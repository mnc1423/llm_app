services:
  llama2-chatbot:
    build:
      context: .
    env_file: .env
    command: streamlit run --server.port 8501 --server.address 0.0.0.0 llama2_chatbot.py
    # command: tail -f /dev/null 
    container_name: llama2-chatbot
    volumes:
      - ./:/app
    ports:
      - "8501:8501"

    deploy:
      resources:
        limits:
          memory: 2G

    networks:
      - elastic 

  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all # Use all available GPUs
  #   volumes:
  #     - ./ollama:/root/.ollama/
  #   networks:
  #     - elastic
      
networks:
  elastic:
    external: true