services:
  llama2-chatbot:
    build:
      context: .            
    env_file: .env
    command: streamlit run main.py --server.port=8501 --server.address=0.0.0.0
    container_name: llama2-chatbot
    volumes:
      - ./app:/app          
    ports:
      - "8501:8501"
    networks:
      - elastic
    deploy:
      resources:
        limits:
          memory: 2G

  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all # Use all available GPUs
  #   volumes:
  #     - ./ollama:/root/.ollama/
  #   networks:
  #     - elastic
      
networks:
  elastic:
    external: true
