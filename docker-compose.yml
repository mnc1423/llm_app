version: '3'
services:
  llama2-chatbot:
    build:
      context: .
    env_file: .env
    command: streamlit run --server.port 8501 --server.address 0.0.0.0 llama2_chatbot.py
    # command: tail -f /dev/null 
    container_name: llama2-chatbot
    volumes:
      - ./:/app
    ports:
      - "8501:8501"

    deploy:
      resources:
        limits:
          memory: 4G

    networks:
      - elastic 

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    networks:
      - elastic
      
networks:
  elastic:
    external: true