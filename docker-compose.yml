services:
  llama2-chatbot:
    build:
      context: .
    env_file: .env
    command: streamlit run --server.port 8501 --server.address 0.0.0.0 Default_Chat.py
    # command: tail -f /dev/null 
    container_name: llama2-chatbot
    volumes:
      - ./:/app
    ports:
      - "8501:8501"
    # networks:
    #   - local_network
    network_mode: "host" # linux setup (security warning)


    deploy:
      resources:
        limits:
          memory: 2G


  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all # Use all available GPUs
  #   volumes:
  #     - ./ollama:/root/.ollama/
  #   networks:
  #     - elastic
# networks:
#   elastic:
#     external: true
networks:
   local_network:
     driver: bridge 

